<link rel="preload" as="style" href="/blog/assets/prism-a11y.css"><link rel="stylesheet" href="/blog/assets/prism-a11y.css"><link rel="stylesheet" href="/blog/assets/topic.css"><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><h2>PyTorch library</h2><aside><nav class="toc"><ol><li><a href="#overview">Overview</a></li><li><a href="#conventions">Conventions</a></li></ol></nav></aside><article><h2 id="overview">Overview</h2><p><a href="https://pytorch.org/" rel="noopener" target="_blank">PyTorch</a> is a deep learning library for Python. It is a port of <a href="http://torch.ch/" rel="noopener" target="_blank">Torch</a> which is implemented in C and Lua. Many PyTorch operations are primarily implemented in C++ and <a href="https://developer.nvidia.com/cuda-zone" rel="noopener" target="_blank">CUDA</a>. CUDA is a language created by NVIDIA that is similar to C++ and supports massive parallelism on graphical processing units (GPUs). Both are frameworks for implementing deep learning algorithms. The first release of PyTorch was in January, 2017.</p><p>Deep learning is a subcategory of artificial intelligence (AI). It involves training neural networks using large amounts of data referred to as a &quot;training set&quot;. The result is a function that accepts input similar to the training set data and outputs something about it. A classic example is taking a photo of a dog and identifying the breed.</p><p>Many applications of deep learning involve image recognition. Other examples include bioinformatics, customer relationship management, demographic predictions fraud detection, game playing, natural language processing, recommendation systems, and speech recognition</p><p>Data is supplied using NumPy data structures.</p><p>&quot;Feature engineering&quot; involves manually determining significant input features to measure, implementing their detection and measurement, and writing an algorithm to combine feature measurements in order to classify inputs. Contrast this to deep learning where feature detection, measurement, and combining measurements is done automatically through training on large sets of inputs. These two approaches, feature engineering and deep learning, can be combined.</p><p>Deep learning typically computes a numerical score for a set of inputs (such as the pixels of an image) and determines the difference between that score and the expected score. Training serves to incrementally lower these differences.</p><p>Currently the most popular alternative to PyTorch is TensorFlow. The code written to use PyTorch tends to be more &quot;pythonic&quot; (idiomatic Python). PyTorch is also regarded as easier to learn than TensorFlow. Their features sets overlap significantly.</p><p>PyTorch provides the &quot;tensor&quot; data structure which is a multidimensional array similar to arrays in NumPy. And just like NumPy, PyTorch implements highly optimized operations on this data structure, that have an API similar to NumPy. This operations are especially fast when run on graphical processing units (GPUs), often providing a speed improvement in the neighborhood of 50 times.</p><p>PyTorch provides a <code>DataLoader</code> class that can load data in the background in preparation for use by the training loop. Each iteration of the training loop evaluates the current model using data from the <code>DataLoader</code>. Model outputs are compared to target outputs using a loss function. The PyTorch &quot;autograd&quot; engine then modifies the model in order to produce outputs that are closer to targets.</p><p>TorchScript can be used to compile models ahead of time. This results in a set of instructions that can be executed in environments that do not use Python such as C++ applications or mobile devices.</p><p>Full training for complex models and large datasets typically require access to a CUDA-capable GPU in order to complete in a reasonable amount of time (hours versus days).</p><p>Some cloud platforms provide online access to Jupyter Notebooks that have PyTorch preinstalled and can process code using GPUs. One example is <a href="https://colab.research.google.com" rel="noopener" target="_blank">Colabortory</a>.</p><h2 id="conventions">Conventions</h2><table><thead><tr><th>Variable Name Suffix</th><th>Type</th></tr></thead><tbody><tr><td><code>_a</code></td><td>NumPy array</td></tr><tr><td><code>_g</code></td><td>GPU memory</td></tr><tr><td><code>_t</code></td><td>tensor</td></tr></tbody></table></article>